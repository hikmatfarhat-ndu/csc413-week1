{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNCSmAeLUHZ2yGjg6jiqqJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/csc413-week1/blob/master/cuda1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFYtmiSnH5Vk"
      },
      "source": [
        "## Example1\n",
        "\n",
        "To write code, create a code cell and write %%WriteFile filename.cu at the beginning. To actually create/modify the file \"run\" the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRvUzFHU3iL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4851559-7859-4af8-849c-d5b5d8210270"
      },
      "source": [
        "%%writefile example1.cu\n",
        "#include \"uda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <iostream>\n",
        "__global__ void kernel(){\n",
        "\n",
        "}\n",
        "int main(){\n",
        "    kernel<<<1,1>>>();\n",
        "std::cout<<\"done\\n\";\n",
        "    \n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting example1.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcYSXgHiIQEW"
      },
      "source": [
        "NVIDIA C++ compiler. The file extension MUST be .cu, otherwise it compiles it with a \"regular\" c++ compiler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYIJIr9oGu0W",
        "outputId": "9b86f560-ae04-41e0-f985-f447c757c08a"
      },
      "source": [
        "!nvcc example1.cu -o example1\n",
        "!./example1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VwGQwEtIcRn"
      },
      "source": [
        "## Example2\n",
        "\n",
        "Recall that before computing on the GPU we need to transfer the data from host memory to device memory. Once the computation is done we transfer it back to the host.\n",
        "Below is a simple example of that process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ii6aU0mIb4A",
        "outputId": "7f22d542-14ac-40eb-ca56-151b983ee50d"
      },
      "source": [
        "%%writefile example2.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "__global__ void kernel(int *x,int *y,int *z){\n",
        "    *z=*x+*y;\n",
        "}\n",
        "int main(){\n",
        "    int a=1,b=2,c=0; //host variables\n",
        "    int *d_a,*d_b,*d_c;//will hold device addresses\n",
        "    // allocate memory for one integer and store the\n",
        "     // address in d_a \n",
        "    cudaMalloc(&d_a,sizeof(int));\n",
        "    cudaMalloc(&d_b,sizeof(int));\n",
        "    cudaMalloc(&d_c,sizeof(int));\n",
        "    // copy the value of a and b\n",
        "    // TO device FROM host\n",
        "    cudaMemcpy(d_a,&a,sizeof(int),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b,&b,sizeof(int),cudaMemcpyHostToDevice);\n",
        "    kernel<<<1,1>>>(d_a,d_b,d_c);\n",
        "    // copy the result TO host FROM device\n",
        "    cudaMemcpy(&c,d_c,sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "std::cout<<\"value of c is \"<<c<<\"\\n\";\n",
        "    \n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing example2.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGlwAN9GHs4h",
        "outputId": "c19c0b67-2ab4-4b0c-f633-cc5b1f2ce0e3"
      },
      "source": [
        "!nvcc example2.cu -o example2\n",
        "!./example2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value of c is 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHdfdpmriw_8"
      },
      "source": [
        "## Example3\n",
        "This is the first example where we use parallelism, computing the sum of two arrays.\n",
        "The computation is performed where each thread computes the sum of two elements. To accomplish that we map the thread id to the array index. In this example we use a __single__, __linear__, block therefore the thread id is equal to the builtin variable threadIdx.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR4-SAOvOdHq",
        "outputId": "3f31da9f-5866-4e64-fe85-9eb40ff2f01c"
      },
      "source": [
        "%%writefile example3.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "__global__ void kernel(float* a, float* b, float* c) {\n",
        "\tint id = threadIdx.x;\n",
        "\tc[id] = a[id] + b[id];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\tint N = 1024;\n",
        "\tfloat* a, * b, * c;\n",
        "\tfloat* da, * db, * dc;\n",
        "  /* allocate memory on host */\n",
        "\ta = (float*)malloc(N * sizeof(float));\n",
        "\tb = (float*)malloc(N * sizeof(float));\n",
        "\tc = (float*)malloc(N * sizeof(float));\n",
        "  /* allocate memory on device */\n",
        "\tcudaMalloc(&da, N * sizeof(float));\n",
        "\tcudaMalloc(&db, N * sizeof(float));\n",
        "\tcudaMalloc(&dc, N * sizeof(float));\n",
        "  /* initialize the arrays a and b */\n",
        "\tfor (int i = 0; i < N; ++i) {\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = 2 * i;\n",
        "\t}\n",
        "  /* copy arrays a and b to device */\n",
        "\tcudaMemcpy(da, a, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\tcudaMemcpy(db, b, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "/* launch kernel with one block of N threads */\n",
        "\tkernel << <1, N >> > (da, db, dc);\n",
        "  /* copy result to host */\n",
        "\tcudaMemcpy(c, dc, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "  /* print the first 10 elements */\n",
        "\tfor (int i = 0; i < 10; ++i)\n",
        "\t\tstd::cout << c[i] << ' ';\n",
        "\tstd::cout << std::endl;\n",
        "\t/* free memory on host and device */\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\tcudaFree(db);\n",
        "\tcudaFree(dc);\n",
        "\tcudaFree(da);\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting example3.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsn9ZgChkn7t",
        "outputId": "6ed92655-b3a1-4b1a-de2c-6e85eb24d9dc"
      },
      "source": [
        "!nvcc example3.cu -o example3\n",
        "!./example3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3 6 9 12 15 18 21 24 27 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rki06JEJliGs"
      },
      "source": [
        "## Thread blocks\n",
        "\n",
        "In CUDA the __maximum__ number of threads in a block is 1024. What if in the previous example we would like to compute the sum of two vectors with size bigger than 1024? We use multiple blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-WqnXdmMti"
      },
      "source": [
        "### Example4\n",
        "We repeat the previous example by using multiple blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPHmBoEkmL1w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
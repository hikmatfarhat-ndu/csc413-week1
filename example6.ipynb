{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+fKbsKwmRiCUf6oRZOPDG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/csc413-week1/blob/master/example6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOMMKWQL67n3",
        "outputId": "b1e8becd-9a81-42f0-dfd8-056417ba8d1d"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-67d7h4g_\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-67d7h4g_\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4308 sha256=5bf696a09a82c1272085da61d4886a7a62b30dd00f063a1ac6a7c34e86076eb5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7978zn7m/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NldfwvGp6-sd",
        "outputId": "e94e92ab-572c-4385-c405-ee004a8d4bfc"
      },
      "source": [
        "%load_ext nvcc_plugin\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGKtfSb26Un0",
        "outputId": "af6c6b51-bbf8-434e-b3db-8e34ce468c66"
      },
      "source": [
        "%%cu\r\n",
        "#include <stdlib.h>\r\n",
        "#include <vector>\r\n",
        "#include <algorithm>\r\n",
        "#include <iostream>\r\n",
        "\r\n",
        "#include <cuda_runtime.h>\r\n",
        "#include <device_launch_parameters.h>\r\n",
        "const  int TILE_WIDTH = 32;\r\n",
        "// a simple version of matrix_multiply which issues redundant loads from off-chip global memory\r\n",
        "__global__ void matrix_multiply_simple(float* a, float* b, float* ab, \r\n",
        "   int w)\r\n",
        "{\r\n",
        "\r\n",
        "    // calculate the row & column index of the element\r\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\r\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
        "    size_t width = w;\r\n",
        "    float result = 0;\r\n",
        "\r\n",
        "    // do dot product between row of a and column of b\r\n",
        "    for (int k = 0; k < w; ++k)\r\n",
        "    {\r\n",
        "        result += a[row * w + k] * b[k * w+ col];\r\n",
        "    }\r\n",
        "\r\n",
        "    // write out this thread's result\r\n",
        "    ab[row * width + col] = result;\r\n",
        "}\r\n",
        "\r\n",
        "// an optimized version of matrix_multiplication which eliminates redundant loads\r\n",
        "__global__ void matrix_multiply(float* a, float* b, float* ab, int width)\r\n",
        "{\r\n",
        "    // create shorthand names for threadIdx & blockIdx\r\n",
        "    int tx = threadIdx.x, ty = threadIdx.y;\r\n",
        "    int bx = blockIdx.x, by = blockIdx.y;\r\n",
        "\r\n",
        "    // allocate 2D tiles in __shared__ memory\r\n",
        "    __shared__ float s_a[TILE_WIDTH][TILE_WIDTH];\r\n",
        "    __shared__ float s_b[TILE_WIDTH][TILE_WIDTH];\r\n",
        "\r\n",
        "    // calculate the row & column index of the element\r\n",
        "    int row = by * blockDim.y + ty;\r\n",
        "    int col = bx * blockDim.x + tx;\r\n",
        "\r\n",
        "    float result = 0;\r\n",
        "    int ntiles = width / TILE_WIDTH;\r\n",
        "    // loop over the tiles of the input in phases\r\n",
        "    for (int p = 0; p < ntiles; ++p)\r\n",
        "    {\r\n",
        "        // collaboratively load tiles into __shared__\r\n",
        "        s_a[ty][tx] = a[row * width + (p * TILE_WIDTH + tx)];\r\n",
        "        s_b[ty][tx] = b[(p * TILE_WIDTH + ty) * width + col];\r\n",
        "\r\n",
        "        // wait until all data is loaded before allowing\r\n",
        "        // any thread in this block to continue\r\n",
        "        __syncthreads();\r\n",
        "\r\n",
        "        // do dot product between row of s_a and column of s_b\r\n",
        "        for (int k = 0; k < TILE_WIDTH; ++k)\r\n",
        "        {\r\n",
        "            result += s_a[ty][k] * s_b[k][tx];\r\n",
        "        }\r\n",
        "\r\n",
        "        // wait until all threads are finished with the data\r\n",
        "        // before allowing any thread in this block to continue\r\n",
        "        __syncthreads();\r\n",
        "    }\r\n",
        "\r\n",
        "    // write out this thread's result\r\n",
        "    ab[row * width + col] = result;\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "int main(void)\r\n",
        "{\r\n",
        "    // create a large workload so we can easily measure the\r\n",
        "    // performance difference of both implementations\r\n",
        "\r\n",
        "    // note that n measures the width of the matrix, not the number of total elements\r\n",
        "    const size_t n = 1 << 10;\r\n",
        "    const dim3 block_size(TILE_WIDTH, TILE_WIDTH);\r\n",
        "    const dim3 num_blocks(n / block_size.x, n / block_size.y);\r\n",
        "\r\n",
        "    // generate random input on the host\r\n",
        "    std::vector<float> h_a(n * n), h_b(n * n), h_c(n * n);\r\n",
        "    for (int i = 0; i < n * n; ++i)\r\n",
        "    {\r\n",
        "        h_a[i] = static_cast<float>(rand()) / RAND_MAX;\r\n",
        "        h_b[i] = static_cast<float>(rand()) / RAND_MAX;\r\n",
        "    }\r\n",
        "\r\n",
        "    // allocate storage for the device\r\n",
        "    float* d_a = 0, * d_b = 0, * d_c = 0;\r\n",
        "    cudaMalloc((void**)&d_a, sizeof(float) * n * n);\r\n",
        "    cudaMalloc((void**)&d_b, sizeof(float) * n * n);\r\n",
        "    cudaMalloc((void**)&d_c, sizeof(float) * n * n);\r\n",
        "\r\n",
        "    // copy input to the device\r\n",
        "    cudaMemcpy(d_a, &h_a[0], sizeof(float) * n * n, cudaMemcpyHostToDevice);\r\n",
        "    cudaMemcpy(d_b, &h_b[0], sizeof(float) * n * n, cudaMemcpyHostToDevice);\r\n",
        "\r\n",
        "    // time the kernel launches using CUDA events\r\n",
        "    cudaEvent_t launch_begin, launch_end;\r\n",
        "    cudaEventCreate(&launch_begin);\r\n",
        "    cudaEventCreate(&launch_end);\r\n",
        "\r\n",
        "    // to get accurate timings, launch a single \"warm-up\" kernel\r\n",
        "    matrix_multiply_simple << <num_blocks, block_size >> > (d_a, d_b, d_c, n);\r\n",
        "\r\n",
        "    // time many kernel launches and take the average time\r\n",
        "    const size_t num_launches = 500;\r\n",
        "    float average_simple_time = 0;\r\n",
        "    std::cout << \"Timing simple implementation...\";\r\n",
        "    for (int i = 0; i < num_launches; ++i)\r\n",
        "    {\r\n",
        "        // record a CUDA event immediately before and after the kernel launch\r\n",
        "        cudaEventRecord(launch_begin, 0);\r\n",
        "        matrix_multiply_simple << <num_blocks, block_size >> > (d_a, d_b, d_c, n);\r\n",
        "        cudaEventRecord(launch_end, 0);\r\n",
        "        cudaEventSynchronize(launch_end);\r\n",
        "\r\n",
        "        // measure the time spent in the kernel\r\n",
        "        float time = 0;\r\n",
        "        cudaEventElapsedTime(&time, launch_begin, launch_end);\r\n",
        "\r\n",
        "        average_simple_time += time;\r\n",
        "    }\r\n",
        "    average_simple_time /= num_launches;\r\n",
        "    std::cout << \" done.\" << std::endl;\r\n",
        "\r\n",
        "    // now time the optimized kernel\r\n",
        "\r\n",
        "    // again, launch a single \"warm-up\" kernel\r\n",
        "    matrix_multiply << <num_blocks, block_size >> > (d_a, d_b, d_c, n);\r\n",
        "\r\n",
        "    // time many kernel launches and take the average time\r\n",
        "    float average_optimized_time = 0;\r\n",
        "    std::cout << \"Timing optimized implementation...\";\r\n",
        "    for (int i = 0; i < num_launches; ++i)\r\n",
        "    {\r\n",
        "        // record a CUDA event immediately before and after the kernel launch\r\n",
        "        cudaEventRecord(launch_begin, 0);\r\n",
        "        matrix_multiply << <num_blocks, block_size >> > (d_a, d_b, d_c, n);\r\n",
        "        cudaEventRecord(launch_end, 0);\r\n",
        "        cudaEventSynchronize(launch_end);\r\n",
        "\r\n",
        "        // measure the time spent in the kernel\r\n",
        "        float time = 0;\r\n",
        "        cudaEventElapsedTime(&time, launch_begin, launch_end);\r\n",
        "\r\n",
        "        average_optimized_time += time;\r\n",
        "    }\r\n",
        "    average_optimized_time /= num_launches;\r\n",
        "    std::cout << \" done.\" << std::endl;\r\n",
        "\r\n",
        "    // report the effective throughput of each kernel in GFLOPS\r\n",
        "    // the effective throughput is measured as the number of floating point operations performed per second:\r\n",
        "    // (one mul + one add) * N^3\r\n",
        "    float simple_throughput = static_cast<float>(2 * n * n * n) / (average_simple_time / 1000.0f) / 1000000000.0f;\r\n",
        "    float optimized_throughput = static_cast<float>(2 * n * n * n) / (average_optimized_time / 1000.0f) / 1000000000.0f;\r\n",
        "\r\n",
        "   \r\n",
        "\r\n",
        "    std::cout << \"Matrix size: \" << n << \"x\" << n << std::endl;\r\n",
        "    std::cout << \"Tile size: \" << TILE_WIDTH << \"x\" << TILE_WIDTH << std::endl;\r\n",
        "    std::cout << \"Simple time : \" << average_simple_time << std::endl;\r\n",
        "    std::cout << \"Optimized time: \" << average_optimized_time << std::endl << std::endl;\r\n",
        "\r\n",
        "    std::cout << \"Throughput of simple kernel: \" << simple_throughput << \" GFLOPS\" << std::endl;\r\n",
        "    std::cout << \"Throughput of optimized kernel: \" << optimized_throughput << \" GFLOPS\" << std::endl;\r\n",
        "    std::cout << \"Performance improvement: \" << optimized_throughput / simple_throughput << \"x\" << std::endl;\r\n",
        "    std::cout << std::endl;\r\n",
        "\r\n",
        "    cudaEventDestroy(launch_begin);\r\n",
        "    cudaEventDestroy(launch_end);\r\n",
        "\r\n",
        "    // deallocate device memory\r\n",
        "    cudaFree(d_a);\r\n",
        "    cudaFree(d_b);\r\n",
        "    cudaFree(d_c);\r\n",
        "\r\n",
        "    return 0;\r\n",
        "}\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timing simple implementation... done.\n",
            "Timing optimized implementation... done.\n",
            "Matrix size: 1024x1024\n",
            "Tile size: 32x32\n",
            "Simple time : 3.52736\n",
            "Optimized time: 2.25987\n",
            "\n",
            "Throughput of simple kernel: 608.807 GFLOPS\n",
            "Throughput of optimized kernel: 950.267 GFLOPS\n",
            "Performance improvement: 1.56087x\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4kQ3CDF7ZNS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}